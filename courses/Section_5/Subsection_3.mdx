---
title: Module 5.3
description: Entropy
version: 1.7.1
author: Kwanwoo Park
transcribedBy: Elizabeth Davis
---

# 5.3 Entropy

---

## 5.3.1 Entropy and the Second Law

**I** spontaneous change: a change that has a tendency to occur without any external influences

<ol type="a">
  <li>
    nonspontaneous change: a change that requires an external influence (work) to occur
  </li>
  <li>
    A tendency to occur is independent of time and rate; thermodynamics $\neq$ kinetics
  </li>
  <li>
    spontaneous changes are not brought about by the overall lowering of energy.
    <ol type="i">
      <li>
        even if the internal energy stays constant (e.g. isothermal free expansion of an
        ideal gas), the change spontaneously occurs.
      </li>
    </ol>
  </li>
  <li>
    spontaneous changes are brought about the tendency of matter and energy to disperse
    <ol type="i">
      <li>
        A spontaneous change will increase disorder, and spread matter randomly
      </li>
    </ol>
  </li>
</ol>

**II** Some examples of spontaneous change are:

<ol type="a">
  <li>
    A hot metal block cools down spontaneously, releasing energy as heat to its
    surroundings. It does not go the opposite way.
  </li>
  <li>
    A concentrated gas spreads around the surroundings spontaneously, but the gas does not
    come together to become concentrated.
  </li>
</ol>

**III** entropy (S): the measure of energy dispersal (disorder)

<ol type="a">
  <li>
    as spontaneous reactions occur, the entropy increases as dispersal increases.
  </li>
  <li>
    a larger entropy represents larger disorder, and a lower entropy represents smaller disorder
  </li>
  <li>
    entropy is also a state function, and its path is irrelevant for calculations.
  </li>
</ol>

**IV** The Second Law of thermodynamics: the entropy of an isolated system tends to increase.

<ol type="a">
  <li>
    often, the isolated system is just taken as the universe, which includes the system of
    interest (which can be open or closed) and its surroundings.
  </li>
  <li>
    the second law of thermodynamics puts restrictions on useful conversions between heat
    and work.
    <ol type="i">
      <li>
        in other words, energy transferred as heat cannot be 100% converted to work
      </li>
    </ol>
  </li>
</ol>

**V** the thermodynamic definition of entropy is defined as:
$\Delta \textrm{S} = \frac{\textrm{q}_{rev}}{T}$ (Eq. 85)

<ol type="a">
  <li>
    These are three physical interpretations of entropy that you should know:
    <ol type="i">
      <li>
        the reversible transfer of energy as heat ensures that it is a smooth, continuous,
        and restrained transfer between two bodies at thermal equilibrium. If the
        transfer of energy as heat were to be nonreversible, it would induce hotspots
        (heterogenous temperatures within the system) which would disperse
        spontaneously, additionally adding to the entropy.
      </li>
      <li>
        the transfer of energy as heat makes usage of disorderly motion, unlike work.
        The degree of disorder in a system is proportional to the energy transfer that
        induces disorderly motion rather than orderly motion.
      </li>
      <li>
        The temperature accounts for the disorder (from thermal motion) that is already
        present within the system. An addition of equivalent heat would have a larger
        impact for a cold system (that has little thermal motion) than a hot system (That
        has a lot of thermal motion) hence T being in the denominator.
      </li>
    </ol>
  </li>
  <li>
    for irreversible processes, a reversible path should be calculated between the two states to
    use Eq. 85.
  </li>
</ol>

**VI** heat engine: a system that converts heat (and chemical energy) to mechanical energy to do work

<ol type="a">
  <li>
    consists of a hot region (source) and a cold region (sink)
    <ol type="i">
      <li>
        If energy from the source is transferred as heat, the entropy
        of the source decreases ($\textrm{q}_{source} \lt 0$)
      </li>
      <li>
        In order for this process to be spontaneous, some heat must
        be transferred to the sink for an overall increase in entropy
      </li>
      <li>
        besides the heat lost to the sink, the rest of the energy
        transferred as heat from the source is converted into
        mechanical energy by the heat engine
      </li>
    </ol>
  </li>

    <img src="https://i.imgur.com/ZA88X3Q.png" />

  <li>
    maximum work is achieved when all energy transfers are processed reversibly
    <ol type="i">
      <li>
        the efficiency of a heat engine can be calculated by ($\eta$, eta):
        $\eta = \frac{\textrm{work produced}}{\textrm{heat absorbed}} = 1 - \frac{\textrm{T}_{cold}}{\textrm{T}_{hot}}$ (Eq. 86)
      </li>
      <li>
        the maximum efficiency ($\eta = 1$) can be achieved when the sink is as cold as
        possible, and the source is as hot as possible.
      </li>
    </ol>
  </li>
  <li>
    A refrigerator works the exact opposite of a typical heat engine
    <ol type="i">
      <li>
        If energy from the cold interior is transferred as heat, its entropy decreases
      </li>
      <li>
        If the same amount of energy were to be transferred into the hot exterior as heat,
        its entropy would increase, but the overall entropy would still be negative
      </li>
      <li>
        to negate this, work is done on the refrigerator to increase the entropy change of
        the warm exterior, and the process of ‘cooling’ the interior would occur.
      </li>
    </ol>
  </li>
</ol>

<img src="https://i.imgur.com/lJQyVIR.png" />

## 5.3.2 Entropy changes

**VII** positional disorder: disorder that arises from the locations of the molecules

<ol type="a">
  <li>
    if the volume of the system increases, the molecules can disperse over a greater volume
    and increases the positional disorder.
  </li>
  <li>
    The entropy change can be calculated for an isothermal expansion of an ideal gas:
    $\Delta \textrm{S} = \textrm{n} \textrm{R} \textrm{ln}(\frac{\textrm{V}_{2}}{\textrm{V}_{1}})$ (Eq. 87)
    <ol type="i">
      <li>
        as entropy is a state function, irreversible functions may use this as well.
      </li>
    </ol>
  </li>
  <li>
    note that temperature is independent of the entropy here. This is because:
    <ol type="i">
      <li>
        As temperature increases, more work is done (internal pressure of the gas
        increases)
      </li>
      <li>
        with more work, more energy must be transferred as heat to maintain the temperature
      </li>
      <li>
        although the temperature is higher, the heat transferred is higher as well, so the
        two effects cancel each other. (Eq. 85)
      </li>
    </ol>
  </li>
  <li>
    Using Boyle’s Law, this equation can be expressed in terms of the change in pressure:
    $\Delta \textrm{S} = \textrm{n} \textrm{R} \textrm{ln}(\frac{\textrm{P}_{2}}{\textrm{P}_{1}})$ (Eq. 88)
    <ol type="i">
      <li>
        for Eq. 89, $\textrm{P}_{1}$ and $\textrm{P}_{2}$ have swapped due to Boyle’s Law stating that pressure is
        inversely proportional to volume.
      </li>
    </ol>
  </li>
</ol>

**VIII** thermal disorder: disorder that arises from the thermal motion of the molecules

<ol type="a">
  <li>
    an increase in temperature (‘heating’) increases thermal order
  </li>
  <li>
    provided the heat capacity is constant over the temperature range of interest,
    $\Delta \textrm{S} = \textrm{C} \textrm{ln}(\frac{\textrm{T}_{2}}{\textrm{T}_{1}})$ (Eq. 89)
    <ol type="i">
      <li>
        in most cases, especially in low temperature solids, the heat capacity is not
        constant. IF this is the case, another approach must be used.
      </li>
      <li>
        there is a better version of this, using calculus. (Ref. Derivation below)
      </li>
    </ol>
  </li>
</ol>

$\Delta \textrm{S} = \textrm{area under the graph of} \, \frac{\textrm{C(T)}}{T}$
plotted against T, between $\textrm{T}_{i}$ and $\textrm{T}_{f}$. (Eq. 90)

<img src="https://i.imgur.com/ggD1Qhu.png" />

## 5.3.3 Entropy and Phases

**IX** entropy of vaporization ($\Delta \textrm{S}_{vap}$): the change in entropy resulting from the vaporization of a liquid

<ol type="a">
  <li>
    The change in entropy resulting from vaporization can be calculated by:
    $\Delta \textrm{S}_{vap} = \frac{\textrm{H}_{vap} \textrm{T}_{b}}{\textrm{T}_{b}}$ (Eq. 91)
  </li>
  <li>
    the equation is slightly different from the normal definition (Eq. 85) as:
    <ol type="i">
      <li>
        at the temperature the phase changes occur, heat transfer occurs reversibly, as
        the system and surroundings are in thermal equilibrium, and any infinitesimal
        change in temperature will lead to a complete phase transition.
      </li>
      <li>
        All the energy transferred as heat is used to drive the phase transition (increase in
        potential energy) rather than raise its temperature (increase in kinetic energy).
      </li>
      <li>
        The transition takes place at constant pressure, and $\Delta \textrm{H}_{vap} = \textrm{q}_{rev}$.
      </li>
    </ol>
  </li>
  <li>
    Trouton’s rule: $\Delta \textrm{S}_{vap} ≈ 85 \, \textrm{J} \textrm{K}^{−1} \textrm{mol}^{−1}$
    for all liquids expect those with strong intermolecular forces
    <ol type="i">
      <li>
        the amount of disorder expected from the dispersion of a compact phase (liquid)
        to a gas that all have very similar molar volumes (ideal gas law) is similar for all liquids
      </li>
    </ol>
  </li>
</ol>

**X** entropy of fusion ($\Delta \textrm{S}_{fus}$): the change in entropy resulting from the melting of a solid

<ol type="a">
  <li>
    The change in entropy resulting from vaporization can be calculated by:
    $\Delta \textrm{S}_{fus} = \frac{\textrm{H}_{fus} \textrm{T}_{f}}{\textrm{T}_{f}}$ (Eq. 92)
  </li>
</ol>

## 5.3.4 The Statistical Definition of Entropy

**XI** Entropy can be considered in terms of the Boltzmann distribution.

<ol type="a">
  <li>
    the distribution of molecules in a range of states is not static, and continuously changes
    over time.
  </li>
  <li>
    The Boltzmann distribution is dependent on temperature; as temperature of the system
    increases, the range of states accessible also increases.
  </li>
  <li>
    At absolute zero, only the ground state is accessible and all molecules are in that state.
  </li>
  <li>
    If entropy is defined as the number of possible configurations (W) a system can have, or
    the distribution of molecules at any instant, it is defined as (k is Boltzmann’s constant):
    $\textrm{S} = \textrm{k} \textrm{ln}(\textrm{W})$ (Eq. 93)
    <ol type="i">
      <li>
        In other words, W (called weight) is the number of possible combinations the
        molecules in the system can be arranged with the same internal energy.
      </li>
      <li>
        statistical entropy: entropy calculated from the Boltzmann Formula (Eq. 93)
      </li>
      <li>
        At absolute zero, W = 1 as there is only one possible configuration; S = 0.
      </li>
    </ol>
  </li>
  <li>
    residual entropy: entropy of a sample at absolute zero arising from positional disorder
    <ol type="i">
      <li>
        the polarity of a molecule can influence can influence a molecules arrangement
      </li>
      <li>
        residual entropy may be very close to zero for very polar molecules, as dipole-
        dipole interactions may encourage the system to be arranged orderly which leads
        to very few configurations with the same low energy
      </li>
    </ol>
  </li>
</ol>

<img src="https://i.imgur.com/DeaePqs.png" />

**XII** The statistical definition of entropy is consistent with the thermodynamic definition of entropy

<ol type="a">
  <li>
    If the system increases in volume, the energy separation between two energy levels decreases
    <ol type="i">
      <li>
        The energy separation of two levels in a particle-in-a-box decreases when the
        length of the box increases. The length of a one-dimensional particle-in-a-box is
        an analog of volume in a three-dimensional particle-in-a-box.
      </li>
      <li>
        As there are more energy levels available, there are more ways to distribute the
        molecules into different energy levels.
      </li>
      <li>
        As there are more energy levels available, there are more ways to distribute the
        molecules into different energy levels.
      </li>
    </ol>
  </li>
  <li>
    the Boltzmann formula also illustrates the relationship between entropy and temperature.
    <ol type="i">
      <li>
        At absolute zero, there is only one configuration as the only energy level allowed
        is the ground state; therefore, entropy must equal zero.
      </li>
      <li>
        As temperature increases, there are more energy levels that are accessible and
        therefore more possible configurations; W increases and entropy increases.
      </li>
    </ol>
  </li>
</ol>

## 5.3.5 The Third Law

**XIII** Third Law of Thermodynamics: As the temperature approaches absolute zero, the entropy of
all perfect crystals approach zero as well.

<ol type="a">
  <li>
    A perfect crystal is one that has no disorder from arrangement and position (spatial disorder)
  </li>
  <li>
    In a perfect crystal, the thermal disorder solely dictates the entropy of the system.
  </li>
  <li>
    the absolute entropy of a substance can be calculated through:
    $\textrm{S(T)} = \textrm{S}(0) + \Delta \textrm{S}$ (heating from 0 to T) = $\Delta \textrm{S}$ (heating from 0 to T) (Eq. 94)
    <ol type="i">
      <li>
        S(0) equals zero for all perfectly order crystalline materials (perfect crystals)
      </li>
      <li>
        the change in entropy that occurs from heating can be calculated by determining
        the heat capacity from absolute zero to T.
      </li>
      <li>
        if a phase transition occurs between the range of temperatures, then the change in
        entropy that occurs from the phase transition must be added.
      </li>
    </ol>
  </li>
</ol>

**XIV** Third-Law entropy ‘S(T)’: All entropy calculated using Eq. 91 with initial temperature as zero

<ol type="a">
  <li>
    standard molar entropy (${\textrm{S}_{m}}^{\Theta}$): the molar entropy of a pure substance at 1 bar
    <ol type="i">
      <li>
        most of the data provided are recorded at temperatures of 298.15 K
      </li>
    </ol>
  </li>
  <li>
    molecules with strong bonds are expected to have low entropies as they are more orderly
  </li>
  <li>
    the molar entropy of gases is expected to be higher than liquids of the same substance
    <ol type="i">
      <li>
        the difference between solids and liquids are a lot smaller
      </li>
    </ol>
  </li>
  <li>
    higher molar mass tends to have higher standard molar entropies
    <ol type="i">
      <li>
        like volume, increasing the mass of the particle in a particle-in-a-box model
        makes the energy separation much smaller
      </li>
      <li>
        this leads to more available energy levels in the Boltzmann distribution; therefore
        more possible configurations (W increases)
      </li>
    </ol>
  </li>
</ol>

## 5.3.6 Entropy changes in the surroundings

**XVI** The process is spontaneous if the total entropy change (system + surroundings) is positive.

$\Delta \textrm{S} = \Delta \textrm{S}_{sys} + \Delta \textrm{S}_{surr}$ (Eq. 96)

<ol type="a">
  <li>
    The entropy of the system can decrease as long as the entropy of the surroundings
    increase much more than the system.
  </li>
  <li>
    the entropy of the surroundings can be defined after consideration of three conditions:
    <ol type="i">
      <li>
        the surroundings are massive and can absorb any amount of heat while
        maintaining the same temperature.
      </li>
      <li>
        In addition, the surroundings always remain at constant pressure regardless of
        what occurs in the system. Therefore, $\Delta \textrm{H}_{sur}$ can be used instead of $\textrm{q}_{sur, rev}$.
      </li>
      <li>
        As enthalpy is a state function, it is independent of how the heat is transferred
        and does not require the path of transfer to be reversible.
      </li>
      <li>
        $\Delta \textrm{H}_{sur} = − \Delta \textrm{H}_{sys}$. Therefore, at constant temperature and pressure,
        $\Delta \textrm{S}_{sur} = - \frac{\Delta \textrm{H}}{\textrm{T}}$ (Eq. 97)
      </li>
    </ol>
  </li>
</ol>

**XVII** Depending on the change in total entropy,

<ol type="a">
  <li>
    if $\Delta \textrm{S}_{tot}$ is greater than zero, the forward process is spontaneous and irreversible.
  </li>
  <li>
    if $\Delta \textrm{S}_{tot}$ is equal to zero, the process is reversible and has no tendency in either direction; it
    is at equilibrium.
  </li>
  <li>
    if $\Delta \textrm{S}_{tot}$ is less than zero, the reverse process is spontaneous and irreversible.
  </li>
</ol>

**XVIII** Clausius inequality: the entropy of an isolated system never decreases

<ol type="a">
  <li>
    as a reversible process produces maximum work, $\textrm{w}_{rev}$ is greater than $\textrm{w}_{irrev}$.
  </li>
  <li>
    when maximum work is done, minimum energy is transferred as heat. $\textrm{q}_{rev} \lt \textrm{q}_{irrev}$.
    <ol type="i">
      <li>
        don’t forget: in an isolated system, $\Delta \textrm{U} = 0$.
      </li>
    </ol>
  </li>
  <li>
    As $\Delta \textrm{S} = \textrm{q}_{rev}$ and $\Delta \textrm{S} \gt \frac{\textrm{q}_{irrev}}{T}$. Therefore, it must be true that $\Delta \textrm{S} \geq \frac{\textrm{q}}{\textrm{T}}$
  </li>
</ol>